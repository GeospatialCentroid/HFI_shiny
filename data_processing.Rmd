# Data Processing and Mapping

```{r}
source("setup.R")
```

## Read in rasters

Local data/ folder 

```{r}
#hfi_1999 <- terra::rast("data/1999_int16.tif")
rasters <- list.files("data/", pattern = ".tif", full.names = TRUE)

# annual and change rasters

change <- terra::rast(rasters[1])

annual <- map(rasters[-1], terra::rast)
```


Get file names to use later on
```{r}
annual_names <- map(rasters[-1], ~str_extract(.x, "(?<=_)\\d{4}(?=_)"))
```


Match extents
```{r}
# Define a common extent (global example)
common_extent <- ext(-180, 180, -90, 90)

# Extend all rasters to union extent
aligned_rasters <- map(annual, function(r) {
  extend(r, common_extent)
})
```


## Tile Creation

Had to run this in terminal after installing gdal (Mac)

```{bash}
gdal_translate -of VRT -ot Byte -scale /Users/caitlinmothes/Desktop/HFI_shiny/data/1999_int16.tif temp.vrt

gdal2tiles temp.vrt
```

Files default saved in Users/caitlinmothes, moved to data/ folder. Folder is 3GB in size though for zoom level 9...

## Leaflet Mapping

Tiles Josh hosted on AGOL

```{r}
# ArcPro Map Tiles ---------
# 
# url <- "https://tiles.arcgis.com/tiles/swlKRWoduvVuwMcX/arcgis/rest/services/TP_1999_3857/MapServer/tile/{z}/{y}/{x}"

# new URL
url <- "https://tiles.arcgis.com/tiles/swlKRWoduvVuwMcX/arcgis/rest/services/TP_1999_3857_12levels/MapServer/tile/{z}/{y}/{x}"

url2 <- "https://tiles.arcgis.com/tiles/swlKRWoduvVuwMcX/arcgis/rest/services/TP_Change_2023_1999_1k/MapServer/tile/{z}/{y}/{x}"


leaflet() %>% 
  addTiles() %>% 
  addTiles(url,
           options = tileOptions(maxNativeZoom = 12))
```

Testing other projections

```{r}

# Create the leaflet map using the custom Albers Equal Area (EPSG:42303)
leaflet(options = leafletOptions(
  crs = leaflet::leafletCRS(
    crsClass = "L.Proj.CRS",
    code = "EPSG:42303",
    proj4def = "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 
                +datum=WGS84 +units=m +no_defs",
    resolutions = c(8192, 4096, 2048, 1024, 512, 256, 128, 64, 32, 16, 8, 4, 2, 1),
    origin = c(0, 0)
  )
)) %>%
  setView(lng = -97.0, lat = 40, zoom = 1) %>%
  addTiles(
    urlTemplate = "https://tile.openstreetmap.org/{z}/{x}/{y}.png",
    options = tileOptions(tms = TRUE)
  )

```

```{r}
north_america <-
  rnaturalearth::countries110 |>
  dplyr::filter(CONTINENT == "North America")

epsg9311 <- leafletCRS(
  crsClass = "L.Proj.CRS",
  code = "EPSG:9311",
  proj4def = "+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs",
  resolutions = 2 ^ (16:7)
)

pal <- leaflet::colorNumeric(palette = "viridis", domain = north_america$POP_EST)

plot_na_map <- function(opts = leafletOptions()) {
  leaflet(north_america, options = opts) %>%
    addPolygons(
      weight = 1,
      color = "#444444",
      opacity = 1,
      fillColor = ~ pal(POP_EST),
      fillOpacity = 0.7,
      smoothFactor = 0.5,
      label = ~ paste(SOVEREIGNT, POP_EST),
      labelOptions = labelOptions(direction = "auto")
    )
}

plot_na_map()
```

# Read in Vector data

Only need to run this once:

```{r}
# Load global country borders

world <- ne_countries(returnclass = "sv", scale = "medium", type = "map_units")


# save to data/ folder
writeVector(world, "data/countries.shp", overwrite = TRUE)


# Import IPCC A5 Reference Regions

# Download the ZIP file
download.file("https://www.ipcc-data.org/documents/ar5/regions/referenceRegions.zip", "data/IPCC_regions.zip", mode = "wb") 

# Unzip the file
unzip("data/IPCC_regions.zip", exdir = "data/IPCC_regions")

# remove the .zip file
file.remove("data/IPCC_regions.zip")



```

Read the files in

```{r}
countries <- terra::vect("data/countries.shp")

ipcc <- terra::vect("data/IPCC_regions/referenceRegions.shp")
```

Add to map

```{r}

countries_sf <- st_as_sf(countries)
  
leaflet() %>% 
  addTiles() %>% 
  addPolygons(data = countries_sf, color = "blue", fillColor = "red", weight = 1, fillOpacity = 0, popup = ~name, group = "Countries") %>% 
  addLayersControl(
    overlayGroups = c("Countries", "Borders"),  # Specify the layers to toggle
    options = layersControlOptions(collapsed = FALSE)  # Control options
  )
```

# Calculate zonal stats

```{r}

# Create a blank raster matching the resolution/extent of r
poly_raster <- rast(aligned_rasters[[1]])


# Rasterize the polygon vector based on IDs
ipcc_poly <- rasterize(ipcc, poly_raster, field = "NAME")

country_poly <- rasterize(countries, poly_raster, field = "name")

# 1.5 minutes
# system.time(
#   zonal_ipcc <- terra::zonal(hfi_1999, ipcc_poly, fun = "mean", na.rm = TRUE)
# )
# 
# system.time(
#   zonal_country <- terra::zonal(hfi_1999, country_poly, fun = "mean", na.rm = TRUE)
# )
```

Calculate over all years

```{r}

zonal_ipcc_all <- map(aligned_rasters, ~ terra::zonal(.x, ipcc_poly, fun = "mean", na.rm = TRUE))

zonal_country_all <- map(aligned_rasters, ~ terra::zonal(.x, country_poly, fun = "mean", na.rm = TRUE))

```

Add names for combining years
```{r}
names(zonal_ipcc_all) <- paste0("mlHFI_", annual_names)

names(zonal_country_all) <- paste0("mlHFI_", annual_names)
```


Combine and clean
```{r}
ipcc_all <- bind_rows(zonal_ipcc_all, .id = "year") %>%
  pivot_wider(names_from = year, 
              values_from = `mlHFI prediction`)

country_all <-  bind_rows(zonal_country_all, .id = "year") %>%
  pivot_wider(names_from = year, 
              values_from = `mlHFI prediction`)
```


Add to polygons and save

```{r}
# countries
countries <- merge(countries, country_all, by = "name")

writeVector(countries, "app/app_data/countries.shp", overwrite = TRUE)

# ipcc regions
ipcc <- merge(ipcc, ipcc_all, by = "NAME")

writeVector(ipcc, "app/app_data/IPCC_regions/referenceRegions.shp", overwrite = TRUE)
```



# Time series

## MS DATA
Test time series workflow and charts on Mississippi region

```{r}
ms_ts <- map(list.files("data/mississippi_ts/", full.names = TRUE), terra::rast)
```

Benchmarking:

```{r}
start <- Sys.time()
# Classify the raster into groups
r_classified <- classify(test, rcl=matrix(c(-Inf, 10, 1,  # From 0-100: class 1
                                       10, 35, 2,  # From 100-200: class 2
                                       35, Inf, 3), # Above 200: class 3
                                     ncol=3, byrow=TRUE))

# Calculate frequencies
freq_table <- freq(r_classified)

# Calculate proportions
freq_table$proportion <- freq_table$count / sum(freq_table$count)

# Display results
print(freq_table)
end <- Sys.time()

end-start #2.405967 secs
```

## Proportion Time series

```{r}
# For a list of already loaded rasters
years <- 1999:2023

# Or for a SpatRaster collection (terra's equivalent of a stack)
#raster_collection <- sprc(ms_ts)

# Process either list or collection
results <- data.frame()
for(i in 1:length(years)) {
  # For a list
  r <- ms_ts[[i]]
  # Or for a collection
  # r <- raster_collection[[i]]
  
  # Continue with classification as in the function above
  rcl <- matrix(c(-Inf, 10, 1, 10, 35, 2, 35, Inf, 3), ncol=3, byrow=TRUE)
  r_classified <- classify(r, rcl=rcl)
  freq_table <- freq(r_classified)
  freq_table$proportion <- freq_table$count / sum(freq_table$count)
  freq_table$year <- years[i]
  freq_table$class <- c("Low", "Medium", "High")[freq_table$value]
  
  results <- rbind(results, freq_table)
}

# Create stacked bar chart

# Convert class to factor with specific order
results$class <- factor(results$class, levels = c("Low", "Medium", "High"))


ggplot(results, aes(x=factor(year), y=proportion, fill=class)) +
  geom_bar(stat="identity", position="stack") +
  scale_fill_manual(values = c(
    "Low" = "#3182bd",     # Darker blue
    "Medium" = "#41ab5d",  # Green
    "High" = "#e6550d"     # Orange-red
  )) +
  labs(title="Proportional Change Over Time",
       x="Year",
       y="Proportion",
       fill="Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  guides(fill = guide_legend(reverse = TRUE)) # Ensure legend order matches factor levels
```

#### plotly

```{r}
# Create color vector for classes
class_colors <- c("Low" = "#1b9e77", "Medium" = "#7570b3", "High" = "#d95f02")

# Create plotly stacked bar chart
plot_ly(
  data = results,
  x = ~year,
  y = ~proportion,
  color = ~class,
  colors = class_colors,
  type = "bar",
  hoverinfo = "text",
  text = ~paste("Class:", class, "<br>Proportion:", round(proportion, 3), "<br>Year:", year)
) %>%
  layout(
    title = "Proportional Change Over Time",
    xaxis = list(
      title = "Year", 
      tickangle = -45,
      tickmode = "array",
      tickvals = years,  # Force all years to be shown
      ticktext = years,  # Labels for the ticks
      tickfont = list(size = 10)  # Smaller font to fit all values
    ),
    yaxis = list(title = "Proportion"),
    barmode = "stack",
    legend = list(title = list(text = "Class")),
    hoverlabel = list(bgcolor = "white"),
    margin = list(b = 100)  # Increased bottom margin to accommodate labels
  )
```

## Heat Map of Density Distributions

```{r}
years <- 1999:2023

freqs <- map2_df(ms_ts, years, function(x, y){
  freq(x) %>% select(value, count) %>% mutate(year = y)
})

# Create heatmap
plot_ly(
  data = freqs,
  x = ~value,
  y = ~year,
  z = ~count,
  type = "heatmap",
  colorscale = "Viridis"
) %>%
  layout(
    title = "Pixel Value Distribution Heatmap by Year",
    xaxis = list(title = "Pixel Value"),
    yaxis = list(
      title = "Year", 
      #tickangle = -45,
      tickmode = "array",
      tickvals = years,  # Force all years to be shown
      ticktext = years,  # Labels for the ticks
      tickfont = list(size = 10)  # Smaller font to fit all values
    )
  )
```

## Ridgeline Plot

```{r}
# Create plot with smoothed transparent curves colored by year
ggplotly(ggplot(freqs, aes(x = value, y = count, color = year, group = year)) +
 geom_line(size = 0.5, alpha = 0.75)+
  scale_color_viridis_c(option = "turbo", direction = 1) +  # Show only min, middle, max years
  scale_y_continuous(labels = scientific_format(digits = 2)) +  # Scientific notation
  scale_x_continuous(breaks = seq(0, max(freqs$value, na.rm = TRUE), by = 10)) +  # X-axis increments of 10
  labs(
    title = "Pixel Value Distribution by Year",
    subtitle = "Smoothed distribution curves with year gradient",
    x = "Pixel Value",
    y = "Count", 
    color = "Year"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.key.height = unit(1, "cm"),
    panel.grid.minor = element_blank(),
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 12),
    legend.title = element_text(size = 12)  # Angled x-axis labels for better readability
  ))
```

### Save data for App

```{r}
write_csv(results, "app/app_data/proportion_timeseries.csv")

write_csv(freqs, "app/app_data/distribution_timeseries.csv")
```

## REAL DATA

